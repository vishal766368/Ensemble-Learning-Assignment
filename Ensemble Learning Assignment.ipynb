{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2497eaf",
   "metadata": {},
   "source": [
    "Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d49a8",
   "metadata": {},
   "source": [
    "Solution 1: Ensemble Learning is a machine learning technique that combines the predictions of multiple models to produce a more accurate and stable result than any single model alone. The key idea behind ensemble learning is that different models may capture different patterns or errors in the data, and by combining them, the overall system can reduce variance, bias, and overfitting. It works on the principle that “a group of weak learners can come together to form a strong learner.” There are mainly three types of ensemble methods — Bagging, Boosting, and Stacking. Bagging reduces variance by training models on different subsets of data (e.g., Random Forest), Boosting reduces bias by giving more focus to difficult samples (e.g., AdaBoost, XGBoost), and Stacking combines different algorithms through a meta-model. Overall, ensemble learning improves the model’s generalization, increases robustness, and gives better predictive performance on unseen data compared to individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21053d4e",
   "metadata": {},
   "source": [
    "Question 2: What is the difference between Bagging and Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e51933e",
   "metadata": {},
   "source": [
    "Solution 2: Bagging and Boosting are two popular ensemble learning techniques, but they differ in how they combine models and handle data. Bagging (Bootstrap Aggregating) focuses on reducing variance by training multiple models independently on random subsets of the training data. Each model gets a random sample (with replacement) from the dataset, and their predictions are averaged (for regression) or voted (for classification). The Random Forest algorithm is a common example of bagging.\n",
    "\n",
    "Boosting, on the other hand, aims to reduce both bias and variance by training models sequentially. Each new model tries to correct the errors made by the previous ones by giving more weight to misclassified or difficult samples. Examples include AdaBoost, Gradient Boosting, and XGBoost. In short, Bagging builds models in parallel to make them stable, while Boosting builds models in sequence to make them stronger and more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d810b75",
   "metadata": {},
   "source": [
    "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b0da28",
   "metadata": {},
   "source": [
    "Solution 3: Bootstrap sampling is a statistical technique used to create multiple random subsets of data from the original dataset with replacement. This means some samples may appear more than once, while others might not appear at all in a particular subset. Each subset is then used to train a separate model.\n",
    "\n",
    "In Bagging methods like Random Forest, bootstrap sampling plays a key role by ensuring that each decision tree is trained on a slightly different version of the data. This introduces diversity among the trees, reducing the chance that all models will make the same errors. When predictions from all trees are combined (using majority voting for classification or averaging for regression), the final result becomes more stable and accurate. Thus, bootstrap sampling helps Random Forest reduce variance, avoid overfitting, and improve generalization performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ed08d",
   "metadata": {},
   "source": [
    "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df21260a",
   "metadata": {},
   "source": [
    "Solution 4: Out-of-Bag (OOB) samples are the data points that are not included in a particular bootstrap sample during the training of models in Bagging methods like Random Forest. Since bootstrap sampling selects data with replacement, on average, about 63% of the data is used for training each model, while the remaining 37% becomes OOB samples.\n",
    "\n",
    "The OOB score is used as a built-in way to evaluate the model’s performance without needing a separate validation set. After training, each model predicts the output for its OOB samples, and these predictions are compared with the actual values to calculate the model’s accuracy or error. The average performance across all models gives the final OOB score. This method provides an unbiased estimate of the model’s accuracy, saves data for training, and helps in monitoring overfitting effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a634eecf",
   "metadata": {},
   "source": [
    "Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774df9d",
   "metadata": {},
   "source": [
    "Solution 5: In a single Decision Tree, feature importance is calculated based on how much each feature contributes to reducing impurity (like Gini impurity or entropy) when splitting the data. The more a feature reduces impurity across the tree’s nodes, the more important it is considered. However, since a single tree can be affected by noise or specific data splits, its feature importance may not always be reliable.\n",
    "\n",
    "In a Random Forest is an ensemble of many decision trees built on different bootstrap samples and random subsets of features. Here, feature importance is calculated by averaging the importance scores of each feature across all trees. This approach provides a more stable and accurate estimate of which features truly influence the predictions. Therefore, while a single Decision Tree gives a basic view of feature importance, Random Forest offers a more generalized and reliable measure by reducing bias and variance in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b4781",
   "metadata": {},
   "source": [
    "Question 6: Write a Python program to:\n",
    "- Load the Breast Cancer dataset using\n",
    "    sklearn.datasets.load_breast_cancer()\n",
    "- Train a Random Forest Classifier\n",
    "- Print the top 5 most important features based on feature importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aab68d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Important Features:\n",
      "worst concave points    0.123350\n",
      "worst perimeter         0.115661\n",
      "worst area              0.105248\n",
      "worst radius            0.102798\n",
      "mean concave points     0.100735\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Training Random Forest Classifier\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Getting feature importance scores\n",
    "importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "\n",
    "# Displaying top 5 important features\n",
    "top_features = importances.sort_values(ascending=False).head(5)\n",
    "print(\"Top 5 Important Features:\")\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d5e3cb",
   "metadata": {},
   "source": [
    "Question 7: Write a Python program to:\n",
    "- Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
    "- Evaluate its accuracy and compare with a single Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bce7e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 1.0\n",
      "Bagging Classifier Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Loading the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Training a single Decision Tree\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Training a Bagging Classifier with Decision Trees\n",
    "bag_model = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "bag_model.fit(X_train, y_train)\n",
    "bag_pred = bag_model.predict(X_test)\n",
    "\n",
    "# Evaluating accuracy\n",
    "dt_acc = accuracy_score(y_test, dt_pred)\n",
    "bag_acc = accuracy_score(y_test, bag_pred)\n",
    "\n",
    "# Printing results\n",
    "print(\"Decision Tree Accuracy:\", dt_acc)\n",
    "print(\"Bagging Classifier Accuracy:\", bag_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be0d965",
   "metadata": {},
   "source": [
    "Question 8: Write a Python program to:\n",
    "- Train a Random Forest Classifier\n",
    "- Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
    "- Print the best parameters and final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36128543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 5, 'n_estimators': 150}\n",
      "Final Accuracy: 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Loading the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Defining the Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Defining hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 3, 5, 7]\n",
    "}\n",
    "\n",
    "# Using GridSearchCV to find best parameters\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Getting best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Evaluating final model on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Final Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d8f497",
   "metadata": {},
   "source": [
    "Question 9: Write a Python program to:\n",
    "- Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
    "- Compare their Mean Squared Errors (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99d50a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regressor MSE: 0.26057775972150127\n",
      "Random Forest Regressor MSE: 0.2607586690027722\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Loading California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Training Bagging Regressor\n",
    "bag_reg = BaggingRegressor(n_estimators=50, random_state=42)\n",
    "bag_reg.fit(X_train, y_train)\n",
    "bag_pred = bag_reg.predict(X_test)\n",
    "bag_mse = mean_squared_error(y_test, bag_pred)\n",
    "\n",
    "# Training Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "rf_pred = rf_reg.predict(X_test)\n",
    "rf_mse = mean_squared_error(y_test, rf_pred)\n",
    "\n",
    "# Comparing Mean Squared Errors\n",
    "print(\"Bagging Regressor MSE:\", bag_mse)\n",
    "print(\"Random Forest Regressor MSE:\", rf_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f97fde",
   "metadata": {},
   "source": [
    "Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to:\n",
    "- Choose between Bagging or Boosting\n",
    "- Handle overfitting\n",
    "- Select base models\n",
    "- Evaluate performance using cross-validation\n",
    "- Justify how ensemble learning improves decision-making in this real-world context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab54a4e",
   "metadata": {},
   "source": [
    "Solution 10:\n",
    "In predicting loan defaults using ensemble techniques, the approach should be systematic. First, to choose between Bagging and Boosting, I would analyze the dataset: if the data is large and noisy, Bagging (e.g., Random Forest) is preferred to reduce variance and prevent overfitting. If the data is smaller or patterns are complex, Boosting (e.g., XGBoost, AdaBoost) is suitable to reduce bias and improve accuracy by focusing on difficult cases.\n",
    "\n",
    "To handle overfitting, I would tune hyperparameters like max_depth, min_samples_split, and the number of estimators, and possibly use regularization techniques like learning rate reduction in boosting. Selecting base models involves choosing algorithms that complement each other; decision trees are common for both Bagging and Boosting, but combining logistic regression or gradient boosting as meta-learners can also be effective.\n",
    "\n",
    "For performance evaluation, I would use **k-fold cross-validation, which provides a reliable estimate of model accuracy and generalization on unseen data.\n",
    "\n",
    "Ensemble learning improves decision-making in this financial context by combining multiple models to increase prediction accuracy, reduce errors, and provide robust risk assessment. This helps the institution make better-informed lending decisions, minimize defaults, and optimize credit allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa5ec9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\datascience\\Anaconda\\conda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "h:\\datascience\\Anaconda\\conda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "h:\\datascience\\Anaconda\\conda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "h:\\datascience\\Anaconda\\conda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "h:\\datascience\\Anaconda\\conda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "h:\\datascience\\Anaconda\\conda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8866666666666667\n",
      "AdaBoost Accuracy: 0.8166666666666667\n",
      "Random Forest CV Score: 0.916\n",
      "AdaBoost CV Score: 0.808\n",
      "Best RF Parameters: {'max_depth': 10, 'n_estimators': 100}\n",
      "Confusion Matrix:\n",
      " [[135  25]\n",
      " [ 12 128]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Step 1: Load your dataset (example CSV, replace with actual file)\n",
    "# For demonstration, we'll create a synthetic dataset\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
    "                           n_redundant=5, n_classes=2, random_state=42)\n",
    "\n",
    "# Step 2: Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Choose ensemble method\n",
    "# Bagging for large/noisy data or Boosting for complex patterns\n",
    "# Here we show both for comparison\n",
    "\n",
    "# Bagging Example: Random Forest\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=None)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "# Boosting Example: AdaBoost\n",
    "ab = AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "ab.fit(X_train, y_train)\n",
    "ab_pred = ab.predict(X_test)\n",
    "ab_acc = accuracy_score(y_test, ab_pred)\n",
    "\n",
    "# Step 4: Evaluate using cross-validation\n",
    "rf_cv = cross_val_score(rf, X, y, cv=5).mean()\n",
    "ab_cv = cross_val_score(ab, X, y, cv=5).mean()\n",
    "\n",
    "# Step 5: Handle overfitting by tuning hyperparameters\n",
    "param_grid_rf = {'n_estimators': [50, 100], 'max_depth': [None, 5, 10]}\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "best_rf = grid_rf.best_estimator_\n",
    "\n",
    "# Step 6: Print results\n",
    "print(\"Random Forest Accuracy:\", rf_acc)\n",
    "print(\"AdaBoost Accuracy:\", ab_acc)\n",
    "print(\"Random Forest CV Score:\", rf_cv)\n",
    "print(\"AdaBoost CV Score:\", ab_cv)\n",
    "print(\"Best RF Parameters:\", grid_rf.best_params_)\n",
    "\n",
    "# Confusion matrix for best RF\n",
    "cm = confusion_matrix(y_test, best_rf.predict(X_test))\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
